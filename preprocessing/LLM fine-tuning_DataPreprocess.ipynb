{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "how an AI assistant knows whether a review sounds positive or negative?\n",
        "The secret lies in preprocessing‚Äîthe essential first step that prepares raw text so a machine learning model can understand it.\n",
        "\n",
        "Think of preprocessing as cleaning and organizing your room before inviting guests. A clean room (or clean dataset) sets the stage for better results.\n",
        "\n",
        "In this walkthrough, imagine you‚Äôre working with patient feedback in a healthcare setting. Before training a sentiment analysis model, you must ensure the text is clean, consistent, and ready for learning."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Steps in Preprocessing\n",
        "\n",
        "We will cover:\n",
        "\n",
        "Clean text\n",
        "\n",
        "Apply tokenization\n",
        "\n",
        "Handle missing data\n",
        "\n",
        "Normalize text\n",
        "\n",
        "Prepare the data for fine-tuning\n",
        "\n",
        "Split the data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Clean Text\n",
        "\n",
        "Cleaning text is the first major step in preparing data for NLP. It removes unnecessary ‚Äúnoise‚Äù that doesn‚Äôt help the model learn.\n",
        "\n",
        "Typical cleaning includes:\n",
        "\n",
        "Removing special characters\n",
        "\n",
        "Removing URLs (http://example‚Ä¶)\n",
        "\n",
        "Removing repeated spaces\n",
        "\n",
        "Converting the text to lowercase"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Create a noisy sample dataset\n",
        "data_dict = {\n",
        "    \"text\": [\n",
        "        \"  The staff was very kind and attentive to my needs!!!  \",\n",
        "        \"The waiting time was too long, and the staff was rude. Visit us at http://hospitalreviews.com\",\n",
        "        \"The doctor answered all my questions...but the facility was outdated.   \",\n",
        "        \"The nurse was compassionate & made me feel comfortable!! :) \",\n",
        "        \"I had to wait over an hour before being seen.  Unacceptable service! #frustrated\",\n",
        "        \"The check-in process was smooth, but the doctor seemed rushed. Visit https://feedback.com\",\n",
        "        \"Everyone I interacted with was professional and helpful. üòä  \"\n",
        "    ],\n",
        "    \"label\": [\"positive\", \"negative\", \"neutral\", \"positive\", \"negative\", \"neutral\", \"positive\"]\n",
        "}\n",
        "\n",
        "# Convert to a DataFrame\n",
        "data = pd.DataFrame(data_dict)\n",
        "\n",
        "# Function to clean the text\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Convert to lowercase\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation and special characters\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra whitespaces\n",
        "    return text\n",
        "\n",
        "# Apply the cleaning function\n",
        "data['cleaned_text'] = data['text'].apply(clean_text)\n",
        "print(data[['cleaned_text', 'label']].head())"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "                                        cleaned_text     label\n0  the staff was very kind and attentive to my needs  positive\n1  the waiting time was too long and the staff wa...  negative\n2  the doctor answered all my questionsbut the fa...   neutral\n3  the nurse was compassionate made me feel comfo...  positive\n4  i had to wait over an hour before being seen u...  negative\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1764849474900
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Cleaning the text ensures that the data provided to the machine learning model is consistent, removing unwanted characters or formatting that could confuse the model. Clean data leads to better feature extraction and, ultimately, improved performance during the training and testing phases. This step is particularly important when fine-tuning LLMs, as clean data ensures the model can focus on learning task-specific patterns."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply tokenization\n",
        "\n",
        "Tokenization is the process of splitting text into individual words or tokens that a model can understand. Tokenization helps break down the text into manageable parts for analysis and learning, especially when working with transformer-based models, such as BERT."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install transformers --quiet\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Requirement already satisfied: pip in /anaconda/envs/azureml_py38/lib/python3.10/site-packages (25.3)\r\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1764849048397
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install transformers --quiet\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install --upgrade pip\u001b[0m\r\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1764849157244
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "print(transformers.__version__)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "4.57.3\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1764849188595
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "!{sys.executable} -m pip install torch --quiet\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\r\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install --upgrade pip\u001b[0m\r\n"
        }
      ],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1764849418861
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "2.9.1+cu128\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1764849456303
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "\n",
        "def tokenize_function(text):\n",
        "    return tokenizer(\n",
        "        text,\n",
        "        padding='max_length',  # pad all sequences to max_length\n",
        "        truncation=True,\n",
        "        max_length=16,         # set a fixed length\n",
        "        return_tensors=\"pt\"\n",
        "    )\n",
        "\n",
        "data['tokenized'] = data['cleaned_text'].apply(tokenize_function)\n"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1764849629665
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is this important?\n",
        "\n",
        "Tokenization transforms raw text into a format that the machine learning model can process. Without tokenization, a model would struggle to interpret the text‚Äôs meaning, particularly for NLP tasks in which understanding individual words and their contexts is critical. Using a task-specific tokenizer ensures compatibility with LLMs like BERT, which will be fine-tuned later in your workflow."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Handle Missing Data\n",
        "\n",
        "Missing data is common in real-world datasets. You must choose how to handle it:\n",
        "\n",
        "Two approaches:\n",
        "\n",
        "Remove rows with missing text\n",
        "\n",
        "Fill missing values (e.g., with \"unknown\")"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check for missing data\n",
        "print(data.isnull().sum())\n",
        "\n",
        "# Option 1: Drop rows with missing data\n",
        "data = data.dropna()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "text            0\nlabel           0\ncleaned_text    0\ntokenized       0\ndtype: int64\n"
        }
      ],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1764849634902
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is this important?\n",
        "\n",
        "Missing data can lead to bias in the model or cause errors during training. By addressing missing data properly, you ensure that your model learns from complete and accurate information, improving its ability to make correct predictions. This is especially crucial when preparing task-specific datasets for fine-tuning, where data quality directly impacts performance"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 4: Normalize Text\n",
        "\n",
        "Normalization standardizes text so the model sees consistent patterns.\n",
        "\n",
        "Typical steps include:\n",
        "\n",
        "Convert text to lowercase\n",
        "\n",
        "Expand contractions (e.g., \"don't\" ‚Üí \"do not\")\n",
        "\n",
        "Correct spelling\n",
        "\n",
        "Remove stop words\n",
        "\n",
        "Apply stemming or lemmatization (reducing words to base form)\n",
        "\n",
        "Additional Techniques\n",
        "\n",
        "Stemming: ‚Äúrunning‚Äù ‚Üí ‚Äúrun‚Äù\n",
        "\n",
        "Lemmatization: ‚Äúbetter‚Äù ‚Üí ‚Äúgood‚Äù\n",
        "\n",
        "Stop word removal: remove words like ‚Äúthe,‚Äù ‚Äúand,‚Äù ‚Äúis‚Äù\n",
        "\n",
        "Normalization helps the model focus on core meaning, not variations."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 5: Prepare the data for fine-tuning\n",
        "\n",
        "After cleaning and tokenizing the text, you must prepare the data for training. In tasks like fine-tuning, structuring the data correctly ensures compatibility with LLMs like BERT. This involves organizing the tokenized data and labels into a format that the machine learning model can use during training, for example, as PyTorch DataLoader objects."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Prepare tensors for fine-tuning\n",
        "input_ids = torch.cat([token['input_ids'] for token in data['tokenized']], dim=0)\n",
        "attention_masks = torch.cat([token['attention_mask'] for token in data['tokenized']], dim=0)\n",
        "\n",
        "labels = torch.tensor([0 if label == \"negative\" else 1 if label == \"neutral\" else 2 for label in data['label']])\n",
        "\n",
        "# Create DataLoader\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
        "\n",
        "print(\"DataLoader created successfully!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "DataLoader created successfully!\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1764849640255
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Structuring the data in this way ensures that the model can efficiently process it during training, allowing for smoother fine-tuning and faster convergence. By preparing your data in this format, you enable the model to handle large datasets effectively, even in real-time applications."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Step 6: Split the data\n",
        "\n",
        "Splitting your dataset into training, validation, and test sets is critical for ensuring your model generalizes well to unseen data. Proper data splitting allows you to monitor the model's performance during training and prevents overfitting, which occurs when the model learns patterns in the training data too well but fails to generalize."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "train_inputs, test_inputs, train_labels, test_labels = train_test_split(\n",
        "    input_ids, labels, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Create DataLoader objects\n",
        "train_dataset = TensorDataset(train_inputs, train_labels)\n",
        "test_dataset = TensorDataset(test_inputs, test_labels)\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=16)\n",
        "\n",
        "print(\"Data splitting successful!\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Data splitting successful!\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1764849644884
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A proper split:\n",
        "\n",
        "Prevents overfitting\n",
        "\n",
        "Gives you a realistic estimate of performance\n",
        "\n",
        "Helps ensure the model will work well on new, unseen data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Preprocessing is one of the most important steps in building machine learning systems‚Äîespecially for NLP tasks.\n",
        "\n",
        "In this walkthrough, you learned how to:\n",
        "\n",
        "Clean and normalize text\n",
        "\n",
        "Tokenize using LLM-friendly tokenizers\n",
        "\n",
        "Handle missing data\n",
        "\n",
        "Structure data for training\n",
        "\n",
        "Split the dataset for reliable evaluation\n",
        "\n",
        "With clean, well-structured data, your fine-tuned model will perform better and produce more reliable predictions."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Code Part         | What It Represents         | Toy Example                    |\n",
        "| ----------------- | -------------------------- | ------------------------------ |\n",
        "| `input_ids`       | Tokenized text             | `[101, 2009, 2293, 7598, 102]` |\n",
        "| `attention_masks` | Marks which tokens matter  | `[1, 1, 1, 1, 1]`              |\n",
        "| `labels`          | The class (sentiment)      | `1`                            |\n",
        "| `TensorDataset`   | Packages these together    | `(input_ids, mask, label)`     |\n",
        "| `DataLoader`      | Feeds batches to the model | Batch size = 8                 |\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}