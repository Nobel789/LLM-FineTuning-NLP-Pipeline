{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# 1. Install PyTorch (CPU Version)\n",
        "%pip install torch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 --index-url https://download.pytorch.org/whl/cpu\n",
        "\n",
        "# 2. Install Hugging Face Libraries\n",
        "%pip install transformers==4.35.2 peft==0.7.1 accelerate==0.25.0 datasets==2.16.1 scikit-learn pandas psutil\n",
        "\n",
        "# 3. Verify Installation\n",
        "import torch\n",
        "print(f\"‚úÖ PyTorch Installed: {torch.__version__}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Looking in indexes: https://download.pytorch.org/whl/cpu\nCollecting torch==2.1.2\n  Using cached https://download.pytorch.org/whl/cpu/torch-2.1.2%2Bcpu-cp310-cp310-linux_x86_64.whl (184.9 MB)\nCollecting torchvision==0.16.2\n  Using cached https://download.pytorch.org/whl/cpu/torchvision-0.16.2%2Bcpu-cp310-cp310-linux_x86_64.whl (1.5 MB)\nCollecting torchaudio==2.1.2\n  Using cached https://download.pytorch.org/whl/cpu/torchaudio-2.1.2%2Bcpu-cp310-cp310-linux_x86_64.whl (1.6 MB)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch==2.1.2) (3.18.0)\nRequirement already satisfied: typing-extensions in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch==2.1.2) (4.14.1)\nCollecting sympy (from torch==2.1.2)\n  Downloading https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch==2.1.2) (3.4.2)\nRequirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch==2.1.2) (3.1.6)\nRequirement already satisfied: fsspec in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch==2.1.2) (2023.10.0)\nRequirement already satisfied: numpy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torchvision==0.16.2) (1.23.5)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torchvision==0.16.2) (2.32.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torchvision==0.16.2) (11.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch==2.1.2) (3.0.2)\nRequirement already satisfied: charset_normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->torchvision==0.16.2) (2025.7.9)\nCollecting mpmath<1.4,>=1.1.0 (from sympy->torch==2.1.2)\n  Downloading https://download.pytorch.org/whl/mpmath-1.3.0-py3-none-any.whl (536 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m536.2/536.2 kB\u001b[0m \u001b[31m24.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading https://download.pytorch.org/whl/sympy-1.14.0-py3-none-any.whl (6.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m175.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: mpmath, sympy, torch, torchvision, torchaudio\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5/5\u001b[0m [torchaudio]5\u001b[0m [torchaudio]]\n\u001b[1A\u001b[2KSuccessfully installed mpmath-1.3.0 sympy-1.14.0 torch-2.1.2+cpu torchaudio-2.1.2+cpu torchvision-0.16.2+cpu\nNote: you may need to restart the kernel to use updated packages.\nCollecting transformers==4.35.2\n  Using cached transformers-4.35.2-py3-none-any.whl.metadata (123 kB)\nCollecting peft==0.7.1\n  Using cached peft-0.7.1-py3-none-any.whl.metadata (25 kB)\nCollecting accelerate==0.25.0\n  Using cached accelerate-0.25.0-py3-none-any.whl.metadata (18 kB)\nCollecting datasets==2.16.1\n  Using cached datasets-2.16.1-py3-none-any.whl.metadata (20 kB)\nRequirement already satisfied: scikit-learn in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (1.7.0)\nRequirement already satisfied: pandas in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (1.5.3)\nRequirement already satisfied: psutil in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (5.9.1)\nRequirement already satisfied: filelock in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.35.2) (3.18.0)\nCollecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.35.2)\n  Using cached huggingface_hub-0.36.0-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: numpy>=1.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.35.2) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.35.2) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.35.2) (6.0.2)\nCollecting regex!=2019.12.17 (from transformers==4.35.2)\n  Using cached regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (40 kB)\nRequirement already satisfied: requests in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.35.2) (2.32.4)\nCollecting tokenizers<0.19,>=0.14 (from transformers==4.35.2)\n  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\nCollecting safetensors>=0.3.1 (from transformers==4.35.2)\n  Using cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\nRequirement already satisfied: tqdm>=4.27 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from transformers==4.35.2) (4.67.1)\nRequirement already satisfied: torch>=1.13.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from peft==0.7.1) (2.1.2+cpu)\nRequirement already satisfied: pyarrow>=8.0.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from datasets==2.16.1) (20.0.0)\nCollecting pyarrow-hotfix (from datasets==2.16.1)\n  Using cached pyarrow_hotfix-0.7-py3-none-any.whl.metadata (3.6 kB)\nCollecting dill<0.3.8,>=0.3.0 (from datasets==2.16.1)\n  Using cached dill-0.3.7-py3-none-any.whl.metadata (9.9 kB)\nCollecting xxhash (from datasets==2.16.1)\n  Using cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\nCollecting multiprocess (from datasets==2.16.1)\n  Using cached multiprocess-0.70.18-py310-none-any.whl.metadata (7.5 kB)\nRequirement already satisfied: fsspec<=2023.10.0,>=2023.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from fsspec[http]<=2023.10.0,>=2023.1.0->datasets==2.16.1) (2023.10.0)\nCollecting aiohttp (from datasets==2.16.1)\n  Using cached aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (8.1 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2) (4.14.1)\nCollecting hf-xet<2.0.0,>=1.1.3 (from huggingface-hub<1.0,>=0.16.4->transformers==4.35.2)\n  Using cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\nRequirement already satisfied: scipy>=1.8.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn) (1.15.3)\nRequirement already satisfied: joblib>=1.2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn) (1.5.1)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from pandas) (2025.2)\nCollecting aiohappyeyeballs>=2.5.0 (from aiohttp->datasets==2.16.1)\n  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\nRequirement already satisfied: aiosignal>=1.4.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.4.0)\nCollecting async-timeout<6.0,>=4.0 (from aiohttp->datasets==2.16.1)\n  Using cached async_timeout-5.0.1-py3-none-any.whl.metadata (5.1 kB)\nRequirement already satisfied: attrs>=17.3.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from aiohttp->datasets==2.16.1) (1.7.0)\nCollecting multidict<7.0,>=4.5 (from aiohttp->datasets==2.16.1)\n  Using cached multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (5.3 kB)\nCollecting propcache>=0.2.0 (from aiohttp->datasets==2.16.1)\n  Using cached propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (13 kB)\nCollecting yarl<2.0,>=1.17.0 (from aiohttp->datasets==2.16.1)\n  Using cached yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (75 kB)\nRequirement already satisfied: idna>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from yarl<2.0,>=1.17.0->aiohttp->datasets==2.16.1) (3.10)\nRequirement already satisfied: six>=1.5 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from python-dateutil>=2.8.1->pandas) (1.17.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers==4.35.2) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers==4.35.2) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from requests->transformers==4.35.2) (2025.7.9)\nRequirement already satisfied: sympy in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1) (1.14.0)\nRequirement already satisfied: networkx in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1) (3.4.2)\nRequirement already satisfied: jinja2 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from torch>=1.13.0->peft==0.7.1) (3.1.6)\nRequirement already satisfied: MarkupSafe>=2.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft==0.7.1) (3.0.2)\nINFO: pip is looking at multiple versions of multiprocess to determine which version is compatible with other requirements. This could take a while.\nCollecting multiprocess (from datasets==2.16.1)\n  Using cached multiprocess-0.70.17-py310-none-any.whl.metadata (7.2 kB)\n  Using cached multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n  Using cached multiprocess-0.70.15-py310-none-any.whl.metadata (7.2 kB)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft==0.7.1) (1.3.0)\nUsing cached transformers-4.35.2-py3-none-any.whl (7.9 MB)\nUsing cached peft-0.7.1-py3-none-any.whl (168 kB)\nUsing cached accelerate-0.25.0-py3-none-any.whl (265 kB)\nUsing cached datasets-2.16.1-py3-none-any.whl (507 kB)\nUsing cached dill-0.3.7-py3-none-any.whl (115 kB)\nUsing cached huggingface_hub-0.36.0-py3-none-any.whl (566 kB)\nUsing cached hf_xet-1.2.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\nUsing cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\nUsing cached aiohttp-3.13.2-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (1.7 MB)\nUsing cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\nUsing cached multidict-6.7.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (241 kB)\nUsing cached yarl-1.22.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (346 kB)\nUsing cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nUsing cached propcache-0.4.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (196 kB)\nUsing cached regex-2025.11.3-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (791 kB)\nUsing cached safetensors-0.7.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (507 kB)\nUsing cached multiprocess-0.70.15-py310-none-any.whl (134 kB)\nUsing cached pyarrow_hotfix-0.7-py3-none-any.whl (7.9 kB)\nUsing cached xxhash-3.6.0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (193 kB)\nInstalling collected packages: xxhash, safetensors, regex, pyarrow-hotfix, propcache, multidict, hf-xet, dill, async-timeout, aiohappyeyeballs, yarl, multiprocess, huggingface-hub, tokenizers, aiohttp, accelerate, transformers, peft, datasets\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m19/19\u001b[0m [datasets]/19\u001b[0m [datasets]ers]ub]]\n\u001b[1A\u001b[2KSuccessfully installed accelerate-0.25.0 aiohappyeyeballs-2.6.1 aiohttp-3.13.2 async-timeout-5.0.1 datasets-2.16.1 dill-0.3.7 hf-xet-1.2.0 huggingface-hub-0.36.0 multidict-6.7.0 multiprocess-0.70.15 peft-0.7.1 propcache-0.4.1 pyarrow-hotfix-0.7 regex-2025.11.3 safetensors-0.7.0 tokenizers-0.15.2 transformers-4.35.2 xxhash-3.6.0 yarl-1.22.0\n\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/anaconda/envs/azureml_py310_sdkv2/bin/python -m pip install --upgrade pip\u001b[0m\nNote: you may need to restart the kernel to use updated packages.\n‚úÖ PyTorch Installed: 2.1.2+cpu\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1764901074693
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import psutil\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import shutil\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from transformers import (\n",
        "    BertTokenizer, \n",
        "    BertForSequenceClassification, \n",
        "    Trainer, \n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from datasets import Dataset\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n",
        "\n",
        "# ==========================================\n",
        "# 1. SETUP & DATA PREPARATION\n",
        "# ==========================================\n",
        "print(\"üöÄ [1/5] Preparing Data...\")\n",
        "\n",
        "# Load subset of 20newsgroups (3 categories for clarity)\n",
        "categories = ['sci.space', 'rec.autos', 'comp.graphics']\n",
        "newsgroups = fetch_20newsgroups(subset='all', categories=categories)\n",
        "data = pd.DataFrame({'text': newsgroups.data, 'label': newsgroups.target})\n",
        "\n",
        "# Split: Train (70%), Val (15%), Test (15%)\n",
        "train_df, temp_df = train_test_split(data, test_size=0.3, random_state=42)\n",
        "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)\n",
        "\n",
        "# Convert to Hugging Face Datasets\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "val_dataset = Dataset.from_pandas(val_df)\n",
        "test_dataset = Dataset.from_pandas(test_df)\n",
        "\n",
        "# Tokenization\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "def tokenize_function(examples):\n",
        "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "print(\"   Tokenizing...\")\n",
        "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
        "val_dataset = val_dataset.map(tokenize_function, batched=True)\n",
        "test_dataset = test_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# CRITICAL: Create a small sample (2.5%) for fast CPU training\n",
        "train_sample = train_dataset.shuffle(seed=42).select(range(int(0.025 * len(train_dataset))))\n",
        "print(f\"‚úÖ Data Ready! Training on {len(train_sample)} samples (CPU Optimized).\")\n",
        "\n",
        "# ==========================================\n",
        "# 2. TRAIN TRADITIONAL MODEL (Baseline)\n",
        "# ==========================================\n",
        "print(\"\\nüöÄ [2/5] Training TRADITIONAL Model (Full Fine-Tuning)...\")\n",
        "\n",
        "model_trad = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "training_args_trad = TrainingArguments(\n",
        "    output_dir='./results_traditional',\n",
        "    num_train_epochs=1,              # 1 Epoch is enough for demo\n",
        "    per_device_train_batch_size=4,   # Small batch for CPU\n",
        "    evaluation_strategy=\"no\",        # Skip eval during training to save time\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-5,\n",
        "    use_cpu=True,                    # Force CPU\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_trad = Trainer(\n",
        "    model=model_trad,\n",
        "    args=training_args_trad,\n",
        "    train_dataset=train_sample,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "\n",
        "trainer_trad.train()\n",
        "\n",
        "# Save Traditional Model\n",
        "save_path_trad = \"./traditional_model\"\n",
        "if os.path.exists(save_path_trad): shutil.rmtree(save_path_trad) # Cleanup old\n",
        "model_trad.save_pretrained(save_path_trad)\n",
        "print(\"‚úÖ Traditional Model Saved.\")\n",
        "\n",
        "# Clean up memory\n",
        "del model_trad, trainer_trad\n",
        "\n",
        "# ==========================================\n",
        "# 3. TRAIN LoRA MODEL (CPU Optimized)\n",
        "# ==========================================\n",
        "print(\"\\nüöÄ [3/5] Training LoRA Model (Efficient)...\")\n",
        "\n",
        "# Load fresh base model\n",
        "model_lora = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)\n",
        "\n",
        "# Define LoRA Config\n",
        "lora_config = LoraConfig(\n",
        "    r=8,                 \n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"], # Target attention layers\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=TaskType.SEQ_CLS,\n",
        ")\n",
        "\n",
        "# Apply LoRA\n",
        "model_lora = get_peft_model(model_lora, lora_config)\n",
        "model_lora.print_trainable_parameters()\n",
        "\n",
        "training_args_lora = TrainingArguments(\n",
        "    output_dir='./results_lora',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=4,\n",
        "    evaluation_strategy=\"no\",\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-4,              # Higher LR for LoRA\n",
        "    use_cpu=True,\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer_lora = Trainer(\n",
        "    model=model_lora,\n",
        "    args=training_args_lora,\n",
        "    train_dataset=train_sample,\n",
        "    eval_dataset=val_dataset,\n",
        "    data_collator=DataCollatorWithPadding(tokenizer=tokenizer),\n",
        ")\n",
        "\n",
        "trainer_lora.train()\n",
        "\n",
        "# Save LoRA Model\n",
        "save_path_lora = \"./lora_model\"\n",
        "if os.path.exists(save_path_lora): shutil.rmtree(save_path_lora)\n",
        "model_lora.save_pretrained(save_path_lora)\n",
        "print(\"‚úÖ LoRA Model Saved.\")\n",
        "\n",
        "# ==========================================\n",
        "# 4. EVALUATION & COMPARISON\n",
        "# ==========================================\n",
        "print(\"\\nüöÄ [4/5] Evaluating Size & Performance...\")\n",
        "\n",
        "def get_dir_size(path):\n",
        "    total = 0\n",
        "    for dirpath, _, filenames in os.walk(path):\n",
        "        for f in filenames:\n",
        "            fp = os.path.join(dirpath, f)\n",
        "            total += os.path.getsize(fp)\n",
        "    return total / (1024 * 1024)\n",
        "\n",
        "# 1. Size Comparison\n",
        "size_trad = get_dir_size(save_path_trad)\n",
        "size_lora = get_dir_size(save_path_lora)\n",
        "base_size = 420.0 # Approx size of BERT base\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"üíæ STORAGE COMPARISON\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Traditional Model: {size_trad:.2f} MB\")\n",
        "print(f\"LoRA Adapter:      {size_lora:.2f} MB\")\n",
        "print(f\"Space Saved:       {size_trad - size_lora:.2f} MB\")\n",
        "print(\"-\" * 40)\n",
        "\n",
        "# 2. Performance Evaluation\n",
        "print(\"\\nRunning Prediction on Test Set...\")\n",
        "preds = trainer_lora.predict(test_dataset)\n",
        "pred_labels = np.argmax(preds.predictions, axis=-1)\n",
        "accuracy = accuracy_score(preds.label_ids, pred_labels)\n",
        "\n",
        "print(\"\\n\" + \"=\"*40)\n",
        "print(\"üèÜ MODEL PERFORMANCE\")\n",
        "print(\"=\"*40)\n",
        "print(f\"Test Accuracy: {accuracy:.4f}\")\n",
        "print(\"-\" * 40)\n",
        "print(classification_report(preds.label_ids, pred_labels, target_names=categories))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 2065/2065 [00:10<00:00, 188.64 examples/s]\nMap: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 442/442 [00:02<00:00, 218.72 examples/s]\nMap:   0%|          | 0/443 [00:00<?, ? examples/s]Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "üöÄ [1/5] Preparing Data...\n‚úÖ Data Ready! Training on 51 samples (CPU Optimized).\n\nüöÄ [2/5] Training TRADITIONAL Model (Full Fine-Tuning)...\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:20, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.143100</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "‚úÖ Traditional Model Saved.\n\nüöÄ [3/5] Training LoRA Model (Efficient)...\ntrainable params: 297,219 || all params: 109,781,766 || trainable%: 0.27073621679578375\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [13/13 00:10, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>1.090300</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "‚úÖ LoRA Model Saved.\n\nüöÄ [4/5] Evaluating Size & Performance...\n\n========================================\nüíæ STORAGE COMPARISON\n========================================\nTraditional Model: 417.67 MB\nLoRA Adapter:      1.15 MB\nSpace Saved:       416.53 MB\n----------------------------------------\n\nRunning Prediction on Test Set...\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": ""
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n/anaconda/envs/azureml_py310_sdkv2/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1706: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "gather": {
          "logged": 1764901204811
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3.10 - SDK v2",
      "language": "python",
      "name": "python310-sdkv2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.18",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}